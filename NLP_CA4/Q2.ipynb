{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NfBIEsNjTOtI",
        "outputId": "7ae34855-c5f4-4563-8fc2-e05a1dfbeea1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.41.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.14.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.23.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.25.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (24.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2024.5.15)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.31.0)\n",
            "Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.19.1)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.4)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.23.0->transformers) (2023.6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.23.0->transformers) (4.11.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2024.2.2)\n",
            "Collecting datasets\n",
            "  Downloading datasets-2.19.1-py3-none-any.whl (542 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m542.0/542.0 kB\u001b[0m \u001b[31m8.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from datasets) (3.14.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (1.25.2)\n",
            "Requirement already satisfied: pyarrow>=12.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (14.0.2)\n",
            "Requirement already satisfied: pyarrow-hotfix in /usr/local/lib/python3.10/dist-packages (from datasets) (0.6)\n",
            "Collecting dill<0.3.9,>=0.3.0 (from datasets)\n",
            "  Downloading dill-0.3.8-py3-none-any.whl (116 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m9.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (2.0.3)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (2.31.0)\n",
            "Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (4.66.4)\n",
            "Collecting xxhash (from datasets)\n",
            "  Downloading xxhash-3.4.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.1/194.1 kB\u001b[0m \u001b[31m11.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting multiprocess (from datasets)\n",
            "  Downloading multiprocess-0.70.16-py310-none-any.whl (134 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m12.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: fsspec[http]<=2024.3.1,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (2023.6.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.9.5)\n",
            "Requirement already satisfied: huggingface-hub>=0.21.2 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.23.1)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from datasets) (24.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (6.0.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (23.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.0.5)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.9.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (4.0.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.21.2->datasets) (4.11.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (2024.2.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2023.4)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n",
            "Installing collected packages: xxhash, dill, multiprocess, datasets\n",
            "Successfully installed datasets-2.19.1 dill-0.3.8 multiprocess-0.70.16 xxhash-3.4.1\n"
          ]
        }
      ],
      "source": [
        "!pip install transformers\n",
        "!pip install datasets"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Part1"
      ],
      "metadata": {
        "id": "OlqwH7okQvQK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Zero Shot Prompting\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
        "from datasets import load_dataset\n",
        "import torch\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "model_name = \"SmartGitiCorp/Persian_Llama3\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "model = AutoModelForSequenceClassification.from_pretrained(model_name)\n",
        "dataset = load_dataset(\"multi_nli\", split='validation_matched[:10%]').shuffle(seed=42)\n",
        "\n",
        "def zero_shot_classification(premise, hypothesis, labels=['true', 'false', 'neither']):\n",
        "    labeled_hypotheses = [f\"{hypothesis} {label}.\" for label in labels]\n",
        "    pairs = [(premise, labeled_hypothesis) for labeled_hypothesis in labeled_hypotheses]\n",
        "\n",
        "    inputs = tokenizer(pairs, padding=True, truncation=True, return_tensors=\"pt\", max_length=512)\n",
        "    with torch.no_grad():\n",
        "        outputs = model(**inputs)\n",
        "    logits = outputs.logits\n",
        "    probabilities = torch.softmax(logits, dim=1)\n",
        "    highest_prob_index = torch.argmax(probabilities, dim=1).cpu().numpy()\n",
        "    return labels[highest_prob_index[0]]\n",
        "\n",
        "correct_labels = []\n",
        "predicted_labels = []\n",
        "\n",
        "for sample in dataset:\n",
        "    premise = sample['premise']\n",
        "    hypothesis = sample['hypothesis']\n",
        "    gold_label = sample['label']\n",
        "    label_map = {0: 'false', 1: 'true', 2: 'neither'}\n",
        "    gold_label_text = label_map[gold_label]\n",
        "\n",
        "    prediction = zero_shot_classification(premise, hypothesis)\n",
        "\n",
        "    correct_labels.append(gold_label_text)\n",
        "    predicted_labels.append(prediction)\n",
        "accuracy = accuracy_score(correct_labels, predicted_labels)\n",
        "print(\"Accuracy on multi_nli validation_matched subset: {:.2f}%\".format(accuracy * 100))"
      ],
      "metadata": {
        "id": "pxaAIXmITSJ3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# One-shot Prompting\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
        "from datasets import load_dataset\n",
        "import torch\n",
        "\n",
        "model_name = \"SmartGitiCorp/Persian_Llama3\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "model = AutoModelForSequenceClassification.from_pretrained(model_name)\n",
        "dataset = load_dataset(\"multi_nli\", split='validation_matched[:1%]').shuffle(seed=42)\n",
        "\n",
        "def one_shot_classification(base_premise, base_hypothesis, base_label, new_premise, new_hypothesis):\n",
        "    prompt = f\"Example: '{base_premise}' is to '{base_hypothesis}' as '{base_label}'.\\n\"\n",
        "    prompt += f\"Question: '{new_premise}' is to '{new_hypothesis}' as\"\n",
        "    inputs = tokenizer(prompt, return_tensors=\"pt\", padding=True, truncation=True, max_length=512).to('cuda')\n",
        "    with torch.no_grad():\n",
        "        outputs = model(**inputs)\n",
        "\n",
        "    logits = outputs.logits\n",
        "    predicted_label_index = torch.argmax(logits, dim=1).cpu().numpy()[0]\n",
        "    return [\"false\", \"true\", \"neither\"][predicted_label_index]\n",
        "\n",
        "base_sample = dataset[0]\n",
        "base_premise = base_sample['premise']\n",
        "base_hypothesis = base_sample['hypothesis']\n",
        "base_label = \"true\"\n",
        "new_sample = dataset[1]\n",
        "new_premise = new_sample['premise']\n",
        "new_hypothesis = new_sample['hypothesis']\n",
        "\n",
        "result = one_shot_classification(base_premise, base_hypothesis, base_label, new_premise, new_hypothesis)\n",
        "print(f\"Given '{new_premise}' is to '{new_hypothesis}', the relationship is: {result}\")"
      ],
      "metadata": {
        "id": "3bygeL6zslxu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Part2"
      ],
      "metadata": {
        "id": "afNzfzTyQdvI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "from transformers import LlamaModel, LlamaConfig, LlamaForSequenceClassification\n",
        "from transformers import Trainer, TrainingArguments\n",
        "from datasets import load_dataset\n",
        "\n",
        "class QLoRaLayer(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super(QLoRaLayer, self).__init__()\n",
        "        self.rank = 16\n",
        "        self.query = nn.Linear(config.hidden_size, self.rank, bias=False)\n",
        "        self.key = nn.Linear(self.rank, config.hidden_size, bias=False)\n",
        "\n",
        "    def forward(self, hidden_states):\n",
        "        low_rank = self.query(hidden_states)\n",
        "        modified_states = self.key(low_rank)\n",
        "        return hidden_states + modified_states\n",
        "\n",
        "class LlamaWithQLoRa(nn.Module):\n",
        "    def __init__(self, model_name):\n",
        "        super(LlamaWithQLoRa, self).__init__()\n",
        "        self.llama = LlamaForSequenceClassification.from_pretrained(model_name)\n",
        "        self.config = self.llama.config\n",
        "        self.q_lora_layers = nn.ModuleList([QLoRaLayer(self.config) for _ in range(self.config.num_hidden_layers)])\n",
        "\n",
        "    def forward(self, input_ids, attention_mask=None, labels=None):\n",
        "        outputs = self.llama(input_ids, attention_mask=attention_mask, labels=labels)\n",
        "        sequence_output = outputs.hidden_states[0]\n",
        "        for q_lora_layer in self.q_lora_layers:\n",
        "            sequence_output = q_lora_layer(sequence_output)\n",
        "\n",
        "        outputs.hidden_states = (sequence_output,) + outputs.hidden_states[1:]\n",
        "        return outputs\n",
        "\n",
        "dataset = load_dataset(\"multi_nli\")\n",
        "train_set = dataset['train']\n",
        "val_set = dataset['validation_matched']\n",
        "model_name = 'SmartGitiCorp/Persian_Llama3'\n",
        "model = LlamaWithQLoRa(model_name)\n",
        "\n",
        "for param in model.llama.parameters():\n",
        "    param.requires_grad = False\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    output_dir='./results',\n",
        "    num_train_epochs=3,\n",
        "    per_device_train_batch_size=8,\n",
        "    warmup_steps=500,\n",
        "    weight_decay=0.01,\n",
        "    evaluation_strategy=\"steps\",\n",
        "    logging_dir='./logs',\n",
        "    logging_steps=10\n",
        ")\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=train_set,\n",
        "    eval_dataset=val_set\n",
        ")\n",
        "\n",
        "trainer.train()\n",
        "\n",
        "eval_results = trainer.evaluate()\n",
        "print(eval_results)"
      ],
      "metadata": {
        "id": "YwuBmUohu32J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Part3\n"
      ],
      "metadata": {
        "id": "IV4s4s0BQq0t"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "from transformers import LlamaModel, LlamaConfig, Trainer, TrainingArguments\n",
        "from datasets import load_dataset\n",
        "\n",
        "class QLoRaLayer(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super(QLoRaLayer, self).__init__()\n",
        "        self.rank = 16\n",
        "        self.query = nn.Linear(config.hidden_size, self.rank, bias=False)\n",
        "        self.key = nn.Linear(self.rank, config.hidden_size, bias=False)\n",
        "\n",
        "    def forward(self, hidden_states):\n",
        "        low_rank = self.query(hidden_states)\n",
        "        modified_states = self.key(low_rank)\n",
        "        return hidden_states + modified_states\n",
        "\n",
        "class LlamaWithLinearAndQLoRa(nn.Module):\n",
        "    def __init__(self, model_name, num_labels):\n",
        "        super(LlamaWithLinearAndQLoRa, self).__init__()\n",
        "        self.llama = LlamaModel.from_pretrained(model_name)\n",
        "        self.config = self.llama.config\n",
        "        self.q_lora_layers = nn.ModuleList([QLoRaLayer(self.config) for _ in range(self.config.num_hidden_layers)])\n",
        "        self.classifier = nn.Linear(self.config.hidden_size, num_labels)\n",
        "\n",
        "    def forward(self, input_ids, attention_mask=None, labels=None):\n",
        "        outputs = self.llama(input_ids=input_ids, attention_mask=attention_mask, output_hidden_states=True)\n",
        "        sequence_output = outputs.hidden_states[-1]\n",
        "        for q_lora_layer in self.q_lora_layers:\n",
        "            sequence_output = q_lora_layer(sequence_output)\n",
        "        logits = self.classifier(sequence_output[:, 0, :])\n",
        "\n",
        "        loss = None\n",
        "        if labels is not None:\n",
        "            loss_fct = nn.CrossEntropyLoss()\n",
        "            loss = loss_fct(logits.view(-1, self.config.num_labels), labels.view(-1))\n",
        "\n",
        "        return {\"loss\": loss, \"logits\": logits}\n",
        "\n",
        "dataset = load_dataset(\"multi_nli\")\n",
        "train_set = dataset['train']\n",
        "val_set = dataset['validation_matched']\n",
        "\n",
        "model_name = 'SmartGitiCorp/Persian_Llama3'\n",
        "num_labels = 3\n",
        "model = LlamaWithLinearAndQLoRa(model_name, num_labels)\n",
        "\n",
        "for param in model.llama.parameters():\n",
        "    param.requires_grad = False\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    output_dir='./results',\n",
        "    num_train_epochs=3,\n",
        "    per_device_train_batch_size=8,\n",
        "    warmup_steps=500,\n",
        "    weight_decay=0.01,\n",
        "    evaluation_strategy=\"steps\",\n",
        "    logging_dir='./logs',\n",
        "    logging_steps=10\n",
        ")\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=train_set,\n",
        "    eval_dataset=val_set\n",
        ")\n",
        "\n",
        "trainer.train()\n",
        "\n",
        "eval_results = trainer.evaluate()\n",
        "print(eval_results)"
      ],
      "metadata": {
        "id": "1zfHjbuIQUBm"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}